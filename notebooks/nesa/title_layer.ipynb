{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b587dc4",
   "metadata": {},
   "source": [
    "# Title Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20870bf",
   "metadata": {},
   "source": [
    "## Carregando os dados e modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e61d1",
   "metadata": {},
   "source": [
    "### Modelo de word embeddings do FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0804791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vec_path = '../../models/fast-text/cc.pt.300.vec'\n",
    "\n",
    "def load_fasttext_vec(path):\n",
    "    vecs = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        first = f.readline().split()\n",
    "\n",
    "        if len(first)==2 and first[0].isdigit():\n",
    "            embed_dim = int(first[1])\n",
    "        else:\n",
    "            word, *vals = first\n",
    "            vecs[word] = np.array(vals, dtype=\"float32\")\n",
    "        for line in f:\n",
    "            word, *vals = line.rstrip().split(\" \")\n",
    "            vecs[word] = np.array(vals, dtype=\"float32\")\n",
    "    return vecs, embed_dim\n",
    "\n",
    "ft_vecs, word_embed_dim = load_fasttext_vec(vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed876f4",
   "metadata": {},
   "source": [
    "### Dataset de tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3860f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4061 entries, 0 to 4060\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   SUMMARY    4061 non-null   object \n",
      " 1   DTSTART    4061 non-null   object \n",
      " 2   DTEND      4061 non-null   object \n",
      " 3   CALENDAR   4061 non-null   object \n",
      " 4   DURATION   4061 non-null   float64\n",
      " 5   CREATED    4061 non-null   object \n",
      " 6   TASK       4034 non-null   object \n",
      " 7   TIME_SLOT  4061 non-null   int64  \n",
      " 8   YEAR_WEEK  4061 non-null   object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 285.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../../data/tasks_summary.csv',\n",
    "    encoding='utf-8',\n",
    "    sep=',',\n",
    ")\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2847e",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d7f3c",
   "metadata": {},
   "source": [
    "### Tokenizando as palavras dos titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3590fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: {'<unk>': 1, 'ir': 2, 'aula': 3, 'academia': 4, 'trabalhar': 5, 'inf': 6, 'manha': 7, 'tomar': 8, 'lanchar': 9, 'fazer': 10, 'assistir': 11, 'almocar': 12, 'cafe': 13, 'tarde': 14, 'pre': 15, 'treino': 16, 'jantar': 17, 'disciplina': 18, 'muay': 19, 'thai': 20, 'sistemas': 21, 'dados': 22, 'software': 23, 'programacao': 24, 'analise': 25, 'teste': 26, 'inteligencia': 27, 'artificial': 28, 'estruturas': 29, 'calculo': 30, 'mat': 31, 'consulta': 32, 'introducao': 33, 'analisadores': 34, 'lexicos': 35, 'sintaticos': 36, 'gerencia': 37, 'projetos': 38, 'informatica': 39, 'psicologo': 40, 'numerica': 41, 'adm': 42, 'financas': 43, 'principios': 44, 'engenharia': 45, 'operacionais': 46, 'probabilidade': 47, 'computacional': 48, 'desenvolvimento': 49, 'jogos': 50, 'apple': 51, 'developer': 52, 'academy': 53, 'computadores': 54, 'interacao': 55, 'humano': 56, 'computador': 57, 'medicao': 58, 'reativos': 59, 'algoritmos': 60, 'orientada': 61, 'objetos': 62, 'redes': 63, 'discretas': 64, 'prova': 65, 'projeto': 66, 'construcao': 67, 'ciencia': 68, 'nan': 69, 'algebra': 70, 'avancadas': 71, 'comemorar': 72, 'aniversario': 73, 'participar': 74, 'bancos': 75, 'gerando': 76, 'startup': 77, 'jogar': 78, 'large': 79, 'language': 80, 'models': 81, 'pilates': 82, 'futevolei': 83, 'cre': 84, 'etica': 85, 'crista': 86, 'banco': 87, 'estudar': 88, 'reuniao': 89, 'consultar': 90, 'tcc': 91, 'autoescola': 92, 'musica': 93, 'avaliacao': 94, 'exame': 95, 'matricula': 96, 'modular': 97, 'atividade': 98, 'festa': 99, 'basico': 100, 'linear': 101, 'puc': 102, 'semana': 103, 'imagem': 104, 'sangue': 105, 'psiquiatra': 106, 'canto': 107, 'trabalho': 108, 'bruno': 109, 'quiropraxia': 110, 'cortar': 111, 'cabelo': 112, 'guitarra': 113, 'conversar': 114, 'estagio': 115, 'dr': 116, 'dermatologista': 117, 'entrevista': 118, 'rio': 119, 'ajuste': 120, 'churrasco': 121, 'viajar': 122, 'escola': 123, 'laboratorio': 124, 'arquitetura': 125, 'extra': 126, 'apresentar': 127, 'exames': 128, 'wwdc': 129, 'sabado': 130, 'descomprimir': 131, 'janeiro': 132, 'sobre': 133, 'vovo': 134, 'niver': 135, 'petropolis': 136, 'auto': 137, 'maple': 138, 'estrutura': 139, 'birusamba': 140, 'verao': 141, 'dentista': 142, 'spinning': 143, 'formatura': 144, 'arrumar': 145, 'itaipava': 146, 'ensino': 147, 'ai': 148, 'salon': 149, 'orientador': 150, 'levar': 151, 'namorada': 152, 'ballet': 153, 'julia': 154, 'goulart': 155, 'caio': 156, 'play': 157, 'time': 158, 'violao': 159, 'centro': 160, 'comparecer': 161, 'conversa': 162, 'pratica': 163, 'detran': 164, 'pedro': 165, 'g': 166, 'probcomp': 167, 'alternativo': 168, 'segundo': 169, 'final': 170, 'cirurgiao': 171, 'ortopedista': 172, 'endoscopia': 173, 'oftalmologista': 174, 'nutricionista': 175, 'gmtk': 176, 'gamejam': 177, 'basquete': 178, 'help': 179, 'baile': 180, 'gaiola': 181, 'ficha': 182, 'facetime': 183, 'alessandra': 184, 'barra': 185, 'shopping': 186, 'manter': 187, 'foco': 188, 'gravar': 189, 'comunicacao': 190, 'intercriar': 191, 'oportunidade': 192, 'guelt': 193, 'desafio': 194, 'crash': 195, 'ss': 196, 'feriado': 197, 'entregar': 198, 'jogo': 199, 'gmkt': 200, 'jam': 201, 'outsystems': 202, 'kick': 203, 'off': 204, 'primeiros': 205, 'passos': 206, 'reserva': 207, 'restaurante': 208, 'desenvolvedor': 209, 'ia': 210, 'monitoria': 211, 'simulacao': 212, 'expositiva': 213, 'ii': 214, 'evento': 215, 'devs': 216, 'pilotis': 217, 'gabi': 218, 'sofia': 219, 'giovanna': 220, 'lopes': 221, 'gi': 222, 'fortes': 223, 'casamento': 224, 'rock': 225, 'in': 226, 'amanda': 227, 'luana': 228, 'maia': 229, 'lara': 230, 'mae': 231, 'show': 232, 'harry': 233, 'styles': 234, 'love': 235, 'on': 236, 'tour': 237, 'isabella': 238, 'felipe': 239, 'laura': 240, 'aninha': 241, 'rapha': 242, 'comprar': 243, 'prime': 244, 'day': 245, 'carol': 246, 'benchimol': 247, 'duda': 248, 'castro': 249, 'marcos': 250, 'secretario': 251, 'barbixas': 252, 'rpg': 253, 'millena': 254, 'lu': 255, 'caroli': 256, 'bernardo': 257, 'renata': 258, 'lucca': 259, 'animar': 260, 'ideias': 261, 'keynote': 262, 'programar': 263, 'cena': 264, 'nick': 265, 'tio': 266, 'programa': 267, 'atlas': 268}\n",
      "Words:\n",
      "['tomar cafe manha', 'lanchar manha', 'almocar', 'lanchar tarde', 'tomar pre treino']\n",
      "[[8, 13, 7], [9, 7], [12], [9, 14], [8, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"a\",\"o\",\"as\",\"os\",\"um\",\"uma\",\"uns\",\"umas\",\n",
    "    \"de\",\"do\",\"da\",\"dos\",\"das\",\n",
    "    \"em\",\"no\",\"na\",\"nos\",\"nas\",\n",
    "    \"para\",\"por\",\"que\",\"e\",\"com\",\"sem\",\n",
    "    \"ao\",\"aos\",\"à\",\"às\",\n",
    "    \"que\"\n",
    "}\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    # lowercase e split simples\n",
    "    words = re.sub(r\"[^\\wçáéíóúâêîôûãõü]+\", \" \", text.lower()).split()\n",
    "    # filtra stop_words e tokens muito curtos, números\n",
    "    cleaned = [\n",
    "        w for w in words\n",
    "        if w not in STOP_WORDS\n",
    "        and len(w) > 1\n",
    "        and not w.isdigit()\n",
    "    ]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "\n",
    "task_titles = df[\"TASK\"].astype(str).tolist()\n",
    "\n",
    "clean_texts = [remove_stopwords(t) for t in task_titles]\n",
    "\n",
    "word_tokenizer = Tokenizer(\n",
    "    num_words=None,         # ou limite de vocabulário\n",
    "    lower=True,             # converte tudo p/ minúsculas\n",
    "    oov_token=\"<unk>\",      # índice fixo p/ OOV\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789'\n",
    ")\n",
    "\n",
    "word_tokenizer.fit_on_texts(clean_texts)\n",
    "\n",
    "tokenized_words = word_tokenizer.word_index\n",
    "print(f'Vocab: {tokenized_words}')\n",
    "\n",
    "word_seqs = word_tokenizer.texts_to_sequences(clean_texts)\n",
    "print(f'Words:\\n{clean_texts[:5]}')\n",
    "print(word_seqs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6595",
   "metadata": {},
   "source": [
    "### Embedding do vocabulário de palavras\n",
    "\n",
    "Se a palavra que estou fazendo embedding não estiver no vocabulário, eu estou deixando seu embedding como zero. Isso é uma escolha por saber que virá o Char-CNN para compensar esses vetores que não estão no vocabulário. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d332cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 269\n",
      "Embedding dimensions: 300\n",
      "Embedding matrix shape: (269, 300)\n",
      "Embedding matrix filled: 76425 non-zero entries\n",
      "Words not found in FastText (zero embeddings): 13\n",
      "Zero words: ['<unk>', 'lexicos', 'sintaticos', 'wwdc', 'birusamba', 'probcomp', 'gmtk', 'gamejam', 'intercriar', 'guelt', 'gmkt', 'outsystems', 'benchimol']\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenized_words) + 1\n",
    "print(f'Number of words: {num_words}')\n",
    "\n",
    "print(f'Embedding dimensions: {word_embed_dim}')\n",
    "\n",
    "word_embed_matrix = np.zeros((num_words, word_embed_dim), dtype=\"float32\")\n",
    "print(f'Embedding matrix shape: {word_embed_matrix.shape}')\n",
    "\n",
    "for word, idx in tokenized_words.items():\n",
    "    if word in ft_vecs:\n",
    "        word_embed_matrix[idx] = ft_vecs[word]\n",
    "\n",
    "print(f'Embedding matrix filled: {np.count_nonzero(word_embed_matrix)} non-zero entries')\n",
    "# Find words that are zero (not in ft_vecs)\n",
    "zero_words = []\n",
    "for word, idx in tokenized_words.items():\n",
    "    if word not in ft_vecs:\n",
    "        zero_words.append(word)\n",
    "\n",
    "print(f'Words not found in FastText (zero embeddings): {len(zero_words)}')\n",
    "print(f'Zero words: {zero_words}')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "word_embeddings = nn.Embedding(\n",
    "    num_embeddings=num_words,\n",
    "    embedding_dim=word_embed_dim,\n",
    "    padding_idx=0\n",
    ")\n",
    "word_embeddings.weight.data.copy_(torch.from_numpy(word_embed_matrix))\n",
    "word_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7339d",
   "metadata": {},
   "source": [
    "## Preparando para o Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88ae2d",
   "metadata": {},
   "source": [
    "### Colocando padding nas sequências\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2e2cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 9\n",
      "Padded shape: (4061, 9)\n",
      "Padded sequences:\n",
      "[[ 8 13  7  0  0  0  0  0  0]\n",
      " [ 9  7  0  0  0  0  0  0  0]\n",
      " [12  0  0  0  0  0  0  0  0]\n",
      " [ 9 14  0  0  0  0  0  0  0]\n",
      " [ 8 15 16  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in word_seqs)\n",
    "print(f'Max sequence length: {max_seq_len}')\n",
    "\n",
    "padded_word_seqs = pad_sequences(\n",
    "    word_seqs,\n",
    "    maxlen=max_seq_len,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0\n",
    ")\n",
    "\n",
    "print(f'Padded shape: {padded_word_seqs.shape}')\n",
    "print(f'Padded sequences:\\n{padded_word_seqs[:5]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7bc9b",
   "metadata": {},
   "source": [
    "### Converter para tensor PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea51fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([4061, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "word_inputs = torch.LongTensor(padded_word_seqs)\n",
    "print(\"Tensor shape:\", word_inputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfb750",
   "metadata": {},
   "source": [
    "### Aplicando os embeddings ao tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde973f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying embeddings: torch.Size([4061, 9, 300])\n"
     ]
    }
   ],
   "source": [
    "embedded_word_inputs = word_embeddings(word_inputs)\n",
    "print('After applying embeddings:', embedded_word_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab49ee",
   "metadata": {},
   "source": [
    "# Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956771d7",
   "metadata": {},
   "source": [
    "### Criando o vocabulário tokenizado de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce04cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vocab: {'<unk>': 1, 'a': 2, ' ': 3, 'r': 4, 'i': 5, 'e': 6, 'd': 7, 'o': 8, 't': 9, 'l': 10, 'n': 11, 'c': 12, 'm': 13, 's': 14, 'u': 15, 'h': 16, 'p': 17, 'f': 18, '1': 19, 'b': 20, '-': 21, 'g': 22, '0': 23, 'j': 24, '3': 25, '6': 26, 'v': 27, 'z': 28, '2': 29, '4': 30, 'y': 31, '7': 32, '8': 33, 'w': 34, '9': 35, 'x': 36, '5': 37, 'q': 38, 'k': 39, '.': 40, '!': 41, '&': 42}\n",
      "Tokenized titles:\n",
      "[['tomar', 'cafe', 'manha'], ['lanchar', 'manha'], ['almocar'], ['lanchar', 'tarde'], ['tomar', 'pre', 'treino']]\n",
      "Character sequences:\n",
      "[[[9, 8, 13, 2, 4], [12, 2, 18, 6], [13, 2, 11, 16, 2]], [[10, 2, 11, 12, 16, 2, 4], [13, 2, 11, 16, 2]], [[2, 10, 13, 8, 12, 2, 4]], [[10, 2, 11, 12, 16, 2, 4], [9, 2, 4, 7, 6]], [[9, 8, 13, 2, 4], [17, 4, 6], [9, 4, 6, 5, 11, 8]]]\n"
     ]
    }
   ],
   "source": [
    "char_tokenizer = Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=True,\n",
    "    oov_token=\"<unk>\",\n",
    "    filters=''\n",
    ")\n",
    "\n",
    "char_tokenizer.fit_on_texts(task_titles)\n",
    "\n",
    "tokenized_chars = char_tokenizer.word_index\n",
    "print(f'Char vocab: {tokenized_chars}')\n",
    "\n",
    "tokenized_titles = [t.split() for t in clean_texts]\n",
    "print(f'Tokenized titles:\\n{tokenized_titles[:5]}')\n",
    "\n",
    "char_seqs = [\n",
    "    [char_tokenizer.texts_to_sequences([word])[0] for word in words]\n",
    "    for words in tokenized_titles\n",
    "]\n",
    "print(f'Character sequences:\\n{char_seqs[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525b302",
   "metadata": {},
   "source": [
    "### Embedding do vocabulário de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27a6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars: 43\n"
     ]
    }
   ],
   "source": [
    "num_chars = len(tokenized_chars) + 1\n",
    "print(f'Number of chars: {num_chars}')\n",
    "\n",
    "char_embed_dim = 50             # dimensão do embedding de cada caractere\n",
    "char_embedding = nn.Embedding(\n",
    "    num_embeddings=num_chars,\n",
    "    embedding_dim=char_embed_dim,\n",
    "    padding_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a4b61",
   "metadata": {},
   "source": [
    "## Preparando para o Char-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7fcd0",
   "metadata": {},
   "source": [
    "### Colocando padding nos caracteres das palavras dos títulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f334cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_char_len = 15, max_seq_len = 9\n",
      "Padded char sequences shape: (4061, 9, 15)\n",
      "Padded char sequences:\n",
      "[[[ 9  8 13  2  4  0  0  0  0  0  0  0  0  0  0]\n",
      "  [12  2 18  6  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [13  2 11 16  2  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      " [[10  2 11 12 16  2  4  0  0  0  0  0  0  0  0]\n",
      "  [13  2 11 16  2  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      " [[ 2 10 13  8 12  2  4  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      " [[10  2 11 12 16  2  4  0  0  0  0  0  0  0  0]\n",
      "  [ 9  2  4  7  6  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      " [[ 9  8 13  2  4  0  0  0  0  0  0  0  0  0  0]\n",
      "  [17  4  6  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 9  4  6  5 11  8  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]]\n"
     ]
    }
   ],
   "source": [
    "max_char_len = max(len(seq) for doc in char_seqs for seq in doc)\n",
    "print(f\"max_char_len = {max_char_len}, max_seq_len = {max_seq_len}\")\n",
    "\n",
    "padded_char_seqs = []\n",
    "for doc in char_seqs:\n",
    "    padded_words = pad_sequences(\n",
    "        doc,\n",
    "        maxlen=max_char_len,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )\n",
    "\n",
    "    if padded_words.shape[0] < max_seq_len:\n",
    "        # Preenche com zeros até o tamanho máximo da sequência\n",
    "        pad_docs = np.zeros((max_seq_len - padded_words.shape[0], max_char_len), dtype=int)\n",
    "        padded_words = np.vstack((padded_words, pad_docs))\n",
    "\n",
    "    else:\n",
    "        padded_words = padded_words[:max_seq_len]\n",
    "\n",
    "    padded_char_seqs.append(padded_words)\n",
    "\n",
    "padded_char_seqs = np.stack(padded_char_seqs)\n",
    "print(f'Padded char sequences shape: {padded_char_seqs.shape}')\n",
    "print(f'Padded char sequences:\\n{padded_char_seqs[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682a9eb",
   "metadata": {},
   "source": [
    "### Converter para tensor PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e28fd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char tensor shape: torch.Size([4061, 9, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "char_inputs = torch.LongTensor(padded_char_seqs)\n",
    "print(\"Char tensor shape:\", char_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dde63",
   "metadata": {},
   "source": [
    "### Aplicando os embeddings ao tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c8d88ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying char embeddings: torch.Size([4061, 9, 15, 50])\n"
     ]
    }
   ],
   "source": [
    "embedded_char_inputs = char_embedding(char_inputs)\n",
    "print('After applying char embeddings:', embedded_char_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1c359",
   "metadata": {},
   "source": [
    "## Aplicando as convoluções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9800749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv outputs shapes: torch.Size([36549, 100, 13]), torch.Size([36549, 100, 12]), torch.Size([36549, 100, 11])\n",
      "Pooled outputs shapes: torch.Size([36549, 100]), torch.Size([36549, 100]), torch.Size([36549, 100])\n",
      "Character CNN representation shape: torch.Size([4061, 9, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# filtros que capturam n‑gramas de caracteres\n",
    "filter_widths = [3, 4, 5]            # tamanhos de “janela” de caracteres\n",
    "num_filters   = [100, 100, 100]      # quantos filtros para cada janela\n",
    "\n",
    "conv3 = nn.Conv1d(in_channels=char_embed_dim, out_channels=100, kernel_size=3)\n",
    "conv4 = nn.Conv1d(in_channels=char_embed_dim, out_channels=100, kernel_size=4)\n",
    "conv5 = nn.Conv1d(in_channels=char_embed_dim, out_channels=100, kernel_size=5)\n",
    "\n",
    "activation = nn.ReLU()\n",
    "\n",
    "B, S, L, D = embedded_char_inputs.size()\n",
    "\n",
    "conv_input = (\n",
    "    embedded_char_inputs\n",
    "    .view(B * S, L, D)\n",
    "    .permute(0, 2, 1)\n",
    ")\n",
    "\n",
    "o3 = activation(conv3(conv_input))\n",
    "o4 = activation(conv4(conv_input))\n",
    "o5 = activation(conv5(conv_input))\n",
    "\n",
    "print(f'Conv outputs shapes: {o3.shape}, {o4.shape}, {o5.shape}')\n",
    "\n",
    "p3 = F.max_pool1d(o3, kernel_size=o3.size(2)).squeeze(2)\n",
    "p4 = F.max_pool1d(o4, kernel_size=o4.size(2)).squeeze(2)\n",
    "p5 = F.max_pool1d(o5, kernel_size=o5.size(2)).squeeze(2)\n",
    "\n",
    "print(f'Pooled outputs shapes: {p3.shape}, {p4.shape}, {p5.shape}')\n",
    "\n",
    "cat = torch.cat([p3, p4, p5], dim=1)\n",
    "\n",
    "char_cnn_dim = sum(num_filters)\n",
    "char_repr = cat.view(B, S, char_cnn_dim)\n",
    "print(f'Character CNN representation shape: {char_repr.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec440199",
   "metadata": {},
   "source": [
    "## Combinar os outputs de words e characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c0ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined.shape: torch.Size([4061, 9, 600])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "combined = torch.cat([embedded_word_inputs, char_repr], dim=2)\n",
    "print(\"combined.shape:\", combined.shape)\n",
    "\n",
    "# Calcular os comprimentos reais das sequências (excluindo padding)\n",
    "lengths = torch.sum(word_inputs != 0, dim=1)\n",
    "print(\"lengths:\", lengths[:10])  # mostrar os primeiros 10 comprimentos\n",
    "\n",
    "# Verificar se todos os comprimentos são válidos (> 0)\n",
    "assert torch.all(lengths > 0), \"Algumas sequências têm comprimento zero\"\n",
    "\n",
    "# Criar packed sequence\n",
    "packed = pack_padded_sequence(\n",
    "    combined,\n",
    "    lengths.cpu(),\n",
    "    batch_first=True,\n",
    "    enforce_sorted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "952998de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real positions — mean, std: 0.5775669813156128 0.37232932448387146\n",
      "Pad positions  — mean, std: 0.017469331622123718 0.02378946542739868\n"
     ]
    }
   ],
   "source": [
    "# supondo que word_inputs e char_repr já existam:\n",
    "\n",
    "# 1a) Máscara de palavras reais (True onde há palavra, False em <pad>)\n",
    "word_mask = (word_inputs != 0)        # shape: (B, S)\n",
    "\n",
    "# 1b) Valores médios de char_repr em posições reais vs paddings\n",
    "real_vals = char_repr[word_mask]      # todo char_repr nos timesteps válidos\n",
    "pad_vals  = char_repr[~word_mask]     # char_repr nos timesteps de padding\n",
    "\n",
    "print(\"Real positions — mean, std:\", real_vals.mean().item(), real_vals.std().item())\n",
    "print(\"Pad positions  — mean, std:\", pad_vals.mean().item(),  pad_vals.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d84f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vec [0,0]: [0.31473273038864136, 0.9037463068962097, 0.30907315015792847, 0.5938639044761658, 0.9759604334831238, 0.7703021168708801, 0.48185187578201294, 0.94762122631073, 0.06664206087589264, 0.3470439016819]\n",
      "Pad  vec [0,3]: [0.0, 0.0, 0.0, 0.0, 0.04989437758922577, 0.052361056208610535, 0.03593650460243225, 0.0, 0.06664206087589264, 0.0]\n",
      "Max abs pad‑vals: 0.07796327769756317\n",
      "conv3.bias: -0.0009105079807341099 0.0469384640455246\n",
      "conv4.bias: -0.0038692683447152376 0.04040978103876114\n",
      "conv5.bias: -0.00031745657906867564 0.039121147245168686\n"
     ]
    }
   ],
   "source": [
    "# 1) Inspecionar alguns vetores de char_repr:\n",
    "#    título 0, palavra 0 (real) vs. título 0, palavra 3 (pad)\n",
    "print(\"Real vec [0,0]:\", char_repr[0,0,:10].tolist())   # primeiros 10 dims\n",
    "print(\"Pad  vec [0,3]:\", char_repr[0,3,:10].tolist())\n",
    "\n",
    "# 2) Verificar valores máximos nos pads:\n",
    "pad_vals = char_repr[~(word_inputs!=0)]  # todos os char_repr onde word_inputs == 0\n",
    "print(\"Max abs pad‑vals:\", pad_vals.abs().max().item())\n",
    "\n",
    "# 3) Conferir se os biases das convoluções não estão gerando offset:\n",
    "print(\"conv3.bias:\", conv3.bias.data.mean().item(), conv3.bias.data.std().item())\n",
    "print(\"conv4.bias:\", conv4.bias.data.mean().item(), conv4.bias.data.std().item())\n",
    "print(\"conv5.bias:\", conv5.bias.data.mean().item(), conv5.bias.data.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8cc6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes da máscara:\n",
      "Max abs pad‑vals: 0.07796327769756317\n",
      "\n",
      "Depois da máscara:\n",
      "Max abs pad‑vals: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Correção: Zerar char_repr em posições de padding\n",
    "# A máscara indica onde há palavras reais (True) vs padding (False)\n",
    "word_mask = (word_inputs != 0)  # shape: (B, S)\n",
    "\n",
    "# Expandir a máscara para a dimensão do char_repr\n",
    "char_mask = word_mask.unsqueeze(-1).expand_as(char_repr)  # shape: (B, S, char_cnn_dim)\n",
    "\n",
    "# Aplicar a máscara para zerar posições de padding\n",
    "char_repr_masked = char_repr * char_mask.float()\n",
    "\n",
    "print(\"Antes da máscara:\")\n",
    "print(\"Max abs pad‑vals:\", char_repr[~word_mask].abs().max().item())\n",
    "\n",
    "print(\"\\nDepois da máscara:\")\n",
    "print(\"Max abs pad‑vals:\", char_repr_masked[~word_mask].abs().max().item())\n",
    "\n",
    "# Atualizar char_repr\n",
    "char_repr = char_repr_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8461735",
   "metadata": {},
   "source": [
    "## Implementando o BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03dca822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined.shape após mascaramento: torch.Size([4061, 9, 600])\n",
      "BiLSTM configurado:\n",
      "  Input size: 600\n",
      "  Hidden size: 256\n",
      "  Num layers: 2\n",
      "  Bidirectional: True\n",
      "  Output size: 512 (bidirectional)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Recombinar com as representações mascaradas\n",
    "combined = torch.cat([embedded_word_inputs, char_repr], dim=2)\n",
    "print(\"combined.shape após mascaramento:\", combined.shape)\n",
    "\n",
    "# Parâmetros do BiLSTM\n",
    "input_size = combined.size(-1)  # word_embed_dim + char_cnn_dim = 300 + 300 = 600\n",
    "hidden_size = 256  # tamanho oculto do LSTM\n",
    "num_layers = 2     # número de camadas\n",
    "dropout = 0.3      # dropout entre camadas\n",
    "\n",
    "# Criar a camada BiLSTM\n",
    "bilstm = nn.LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    bidirectional=True,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "print(f\"BiLSTM configurado:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Num layers: {num_layers}\")\n",
    "print(f\"  Bidirectional: True\")\n",
    "print(f\"  Output size: {hidden_size * 2} (bidirectional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f115dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição dos comprimentos: tensor([   0, 1052, 1437,  769,  186,  420,  196,    0,    0,    1])\n",
      "LSTM output shape: torch.Size([4061, 9, 512])\n",
      "Hidden state shape: torch.Size([4, 4061, 256])\n",
      "Cell state shape: torch.Size([4, 4061, 256])\n"
     ]
    }
   ],
   "source": [
    "# Calcular comprimentos das sequências (sem padding)\n",
    "lengths = torch.sum(word_inputs != 0, dim=1)\n",
    "print(\"Distribuição dos comprimentos:\", torch.bincount(lengths))\n",
    "\n",
    "# Empacotar sequências para eficiência no LSTM\n",
    "packed_input = pack_padded_sequence(\n",
    "    combined,\n",
    "    lengths.cpu(),\n",
    "    batch_first=True,\n",
    "    enforce_sorted=False\n",
    ")\n",
    "\n",
    "# Passar pelo BiLSTM\n",
    "packed_output, (hidden, cell) = bilstm(packed_input)\n",
    "\n",
    "# Desempacotar a saída\n",
    "lstm_output, output_lengths = pad_packed_sequence(\n",
    "    packed_output,\n",
    "    batch_first=True,\n",
    "    total_length=max_seq_len  # manter o tamanho original\n",
    ")\n",
    "\n",
    "print(f\"LSTM output shape: {lstm_output.shape}\")\n",
    "print(f\"Hidden state shape: {hidden.shape}\")\n",
    "print(f\"Cell state shape: {cell.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300d2b7",
   "metadata": {},
   "source": [
    "## Camadas de Saída e Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTANDO POOLINGS ===\n",
      "Mean pooled shape: torch.Size([4061, 512])\n",
      "Max pooled shape: torch.Size([4061, 512])\n",
      "Attention pooled shape: torch.Size([4061, 512])\n",
      "Attention weights shape: torch.Size([4061, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Diferentes estratégias de pooling\n",
    "# 1. Mean pooling (ignorando padding)\n",
    "def mean_pooling(sequence_output, lengths):\n",
    "    batch_size, max_len, hidden_dim = sequence_output.size()\n",
    "\n",
    "    # Criar máscara para ignorar padding\n",
    "    mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    mask = mask.float().unsqueeze(-1).to(sequence_output.device)\n",
    "\n",
    "    # Aplicar máscara e calcular média\n",
    "    masked_output = sequence_output * mask\n",
    "    pooled = masked_output.sum(dim=1) / lengths.unsqueeze(-1).float()\n",
    "\n",
    "    return pooled\n",
    "\n",
    "# 2. Max pooling\n",
    "def max_pooling(sequence_output, lengths):\n",
    "    batch_size, max_len, hidden_dim = sequence_output.size()\n",
    "\n",
    "    # Criar máscara para ignorar padding\n",
    "    mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    mask = mask.float().unsqueeze(-1).to(sequence_output.device)\n",
    "\n",
    "    # Aplicar máscara (colocar -inf em posições de padding)\n",
    "    masked_output = sequence_output.masked_fill(mask == 0, float('-inf'))\n",
    "    pooled, _ = masked_output.max(dim=1)\n",
    "\n",
    "    return pooled\n",
    "\n",
    "# 3. Attention pooling (simples)\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, sequence_output, lengths):\n",
    "        batch_size, max_len, hidden_dim = sequence_output.size()\n",
    "\n",
    "        # Calcular scores de atenção\n",
    "        attention_scores = self.attention(sequence_output).squeeze(-1)  # (B, S)\n",
    "\n",
    "        # Criar máscara para ignorar padding\n",
    "        mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # Softmax para obter pesos\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(-1)  # (B, S, 1)\n",
    "\n",
    "        # Weighted sum\n",
    "        pooled = (sequence_output * attention_weights).sum(dim=1)  # (B, hidden_dim)\n",
    "\n",
    "        return pooled, attention_weights.squeeze(-1)\n",
    "\n",
    "# Testando diferentes poolings\n",
    "print(\"=== TESTANDO POOLINGS ===\")\n",
    "\n",
    "# Mean pooling\n",
    "mean_pooled = mean_pooling(lstm_output, lengths)\n",
    "print(f\"Mean pooled shape: {mean_pooled.shape}\")\n",
    "\n",
    "# Max pooling\n",
    "max_pooled = max_pooling(lstm_output, lengths)\n",
    "print(f\"Max pooled shape: {max_pooled.shape}\")\n",
    "\n",
    "# Attention pooling\n",
    "attention_pooler = AttentionPooling(lstm_output.size(-1))\n",
    "att_pooled, att_weights = attention_pooler(lstm_output, lengths)\n",
    "print(f\"Attention pooled shape: {att_pooled.shape}\")\n",
    "print(f\"Attention weights shape: {att_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d286a8",
   "metadata": {},
   "source": [
    "## Camada Final de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01e66953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes (calendários): 11\n",
      "Dimensão após pooling: 512\n",
      "Logits shape: torch.Size([4061, 11])\n",
      "Probabilities shape: torch.Size([4061, 11])\n",
      "\n",
      "Primeiras 5 predições (índices das classes):\n",
      "tensor([6, 6, 6, 6, 6])\n",
      "\n",
      "Primeiras 5 probabilidades máximas:\n",
      "tensor([0.0975, 0.0981, 0.0966, 0.0983, 0.0998], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros para classificação\n",
    "num_classes = len(df['CALENDAR'].unique())  # número de calendários únicos\n",
    "pooled_dim = hidden_size * 2  # BiLSTM output dimension\n",
    "dropout_rate = 0.5\n",
    "\n",
    "print(f\"Número de classes (calendários): {num_classes}\")\n",
    "print(f\"Dimensão após pooling: {pooled_dim}\")\n",
    "\n",
    "# Camada de classificação\n",
    "classifier = nn.Sequential(\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(pooled_dim, pooled_dim // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(pooled_dim // 2, num_classes)\n",
    ")\n",
    "\n",
    "# Testar a classificação com mean pooling\n",
    "logits = classifier(mean_pooled)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Aplicar softmax para obter probabilidades\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "\n",
    "# Mostrar algumas predições\n",
    "print(f\"\\nPrimeiras 5 predições (índices das classes):\")\n",
    "predictions = torch.argmax(probabilities, dim=1)\n",
    "print(predictions[:5])\n",
    "\n",
    "print(f\"\\nPrimeiras 5 probabilidades máximas:\")\n",
    "max_probs, _ = torch.max(probabilities, dim=1)\n",
    "print(max_probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1025481",
   "metadata": {},
   "source": [
    "## Resumo da Arquitetura Completa\n",
    "\n",
    "A arquitetura implementada consiste em:\n",
    "\n",
    "### 1. **Word Embeddings (FastText)**\n",
    "- Embeddings pré-treinados de 300 dimensões\n",
    "- Vocabulário: ~{num_words} palavras\n",
    "- Palavras não encontradas no FastText são zeradas (compensadas pelo Char-CNN)\n",
    "\n",
    "### 2. **Character-CNN**\n",
    "- Embeddings de caracteres: 50 dimensões\n",
    "- 3 filtros convolucionais: tamanhos 3, 4, 5 (100 filtros cada)\n",
    "- Max pooling para capturar n-gramas mais importantes\n",
    "- Representação final: 300 dimensões (100 + 100 + 100)\n",
    "- **Correção aplicada**: Mascaramento de posições de padding\n",
    "\n",
    "### 3. **Concatenação**\n",
    "- Word embeddings (300) + Char-CNN (300) = 600 dimensões\n",
    "- Cada token é representado por ambas as informações\n",
    "\n",
    "### 4. **BiLSTM**\n",
    "- 2 camadas bidirecionais\n",
    "- Hidden size: 256 (512 total por ser bidirecional)\n",
    "- Dropout: 0.3 entre camadas\n",
    "- Packed sequences para eficiência\n",
    "\n",
    "### 5. **Pooling**\n",
    "- Mean pooling: média ponderada ignorando padding\n",
    "- Max pooling: máximo ignorando padding  \n",
    "- Attention pooling: atenção aprendida\n",
    "\n",
    "### 6. **Classificação**\n",
    "- Camada linear: 512 → 256 → {num_classes} classes\n",
    "- Dropout: 0.5 para regularização\n",
    "- Saída: probabilidades para cada calendário\n",
    "\n",
    "Esta arquitetura é típica para **classificação de sequências** onde você precisa:\n",
    "- Capturar semântica (word embeddings)\n",
    "- Capturar morfologia (char-CNN)\n",
    "- Modelar dependências temporais (BiLSTM)\n",
    "- Fazer classificação final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-task-scheduler-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
